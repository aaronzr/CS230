{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH4WkBlpeYJX"
   },
   "source": [
    "# CS230 Project Milestone\n",
    "Aaron Reed (aaron73@stanford.edu)\n",
    "\n",
    "Ivan Villa-Renteria (ivillar@stanford.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3aFn2IZeYJZ"
   },
   "source": [
    "## Objective\n",
    "\n",
    "The ultimate goal of our project is to produce summaries of psychotherapy sessions to aid therapists and their clients.  As a preliminary milestone, we intended to apply an *unsupervised* summarization pipeline proposed by [Padmakumar and Saran](https://www.cs.utexas.edu/~asaran/reports/summarization.pdf) and implemented by [Chauhan](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1) to psychotherapy data. Our plan was to use this unsupervised method as a baseline and compare it with deep learning methods using ROUGE-2 scores. However, we have been unable to access the [psychotherapy transcripts dataset](https://alexanderstreet.com/products/counseling-and-psychotherapy-transcripts-series) we originally intended to use. Here we use a subset of the [Enron Email Dataset](https://www.cs.cmu.edu/~enron/) for the purpose of pipeline development.\n",
    "\n",
    "The following is a representative sample email from the Enron dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HwYKj_ZjWIE",
    "outputId": "9b482097-6fc5-4bc4-a357-6ebc52712713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <11578703.1075855681711.JavaMail.evans@thyme>\r",
      "\r\n",
      "Date: Tue, 12 Sep 2000 06:42:00 -0700 (PDT)\r",
      "\r\n",
      "From: phillip.allen@enron.com\r",
      "\r\n",
      "To: bs_stone@yahoo.com\r",
      "\r\n",
      "Subject: Re: Sept 1 Payment\r",
      "\r\n",
      "Mime-Version: 1.0\r",
      "\r\n",
      "Content-Type: text/plain; charset=us-ascii\r",
      "\r\n",
      "Content-Transfer-Encoding: 7bit\r",
      "\r\n",
      "X-From: Phillip K Allen\r",
      "\r\n",
      "X-To: Brenda Stone <bs_stone@yahoo.com> @ ENRON\r",
      "\r\n",
      "X-cc: \r",
      "\r\n",
      "X-bcc: \r",
      "\r\n",
      "X-Folder: \\Phillip_Allen_Dec2000\\Notes Folders\\Sent\r",
      "\r\n",
      "X-Origin: Allen-P\r",
      "\r\n",
      "X-FileName: pallen.nsf\r",
      "\r\n",
      "\r",
      "\r\n",
      "Brenda,\r\n",
      "\r\n",
      " I checked my records and I mailed check #1178 for the normal amount on \r\n",
      "August 28th.  I mailed it to 4303 Pate Rd. #29, College Station, TX 77845.  I \r\n",
      "will go ahead and mail you another check.  If the first one shows up you can \r\n",
      "treat the 2nd as payment for October.\r\n",
      "\r\n",
      " I know your concerns about the site plan.  I will not proceed without \r\n",
      "getting the details and getting your approval.\r\n",
      "\r\n",
      " I will find that amortization schedule and send it soon.\r\n",
      "\r\n",
      "Phillip"
     ]
    }
   ],
   "source": [
    "!cat sample_email.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RGF2aHWjJAL"
   },
   "source": [
    "## Pipeline walkthrough\n",
    "### Overview\n",
    "\n",
    "The summarization pileline is summarized as follows:\n",
    "\n",
    "\n",
    "1.   Data cleaning*\n",
    "2.   Sentence tokenization\n",
    "3.   Skip-thought encoding\n",
    "4.   Clustering\n",
    "5.   Summarization\n",
    "\n",
    "\\* Chauhan includes langauge detection between cleaning and tokenization. However, since our psychotherapy data will be in English, we omit this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDyKC5v2eYJc"
   },
   "source": [
    "### Install and import\n",
    "The `setup.sh` script downloads about 5GB of model parameters the first time it is run. Please be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wp4rPun_eYJd",
    "outputId": "7390bc1e-4a20-4d0e-9ce4-4ed23fd51ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.7/site-packages (1.0.8)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from langdetect) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aaronreed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!./setup.sh # This downloads model parameters. Wait about 5 minutes.\n",
    "\n",
    "import numpy as np\n",
    "import langdetect\n",
    "# from talon.signature.bruteforce import extract_signature\n",
    "import nltk\n",
    "nltk.download('punkt') # for tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import skipthoughts.skipthoughts as skipthoughts\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u__lRIj_tMTK"
   },
   "source": [
    "### 1. Data cleaning\n",
    "In this step, an open-source [utility](https://github.com/mailgun/talon) is used to strip headers and signatures from emails, since they do not contribute semantic data relevant to summarization. The resulting text is the body portion of the email.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZeKqoYFPrmZs"
   },
   "outputs": [],
   "source": [
    "kNumMetadataLines = 16 # throw out this many lines from header\n",
    "\n",
    "def preprocess(emails):\n",
    "    \"\"\"\n",
    "    Performs preprocessing operations such as:\n",
    "        1. Removing metadata lines.\n",
    "        2. Removing new line characters.\n",
    "    \"\"\"\n",
    "    n_emails = len(emails)\n",
    "    for i in range(n_emails):\n",
    "        email = emails[i]\n",
    "        # email, _ = extract_signature(email)\n",
    "        lines = email.split('\\n')\n",
    "        if len(lines) > kNumMetadataLines:\n",
    "            lines = lines[kNumMetadataLines:] # remove metadata lines\n",
    "        for j in reversed(range(len(lines))):\n",
    "            lines[j] = lines[j].strip()\n",
    "            if lines[j] == '':\n",
    "                lines.pop(j)\n",
    "        emails[i] = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9IXdE50waLB"
   },
   "source": [
    "### 2. Sentence tokenization\n",
    "The body of the email is split into individual sentences which will be encoded into skip-thought vectors in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1y_5wnK3t_FU"
   },
   "outputs": [],
   "source": [
    "def split_sentences(emails):\n",
    "    \"\"\"\n",
    "    Splits the emails into individual sentences\n",
    "    \"\"\"\n",
    "    n_emails = len(emails)\n",
    "    for i in range(n_emails):\n",
    "        email = emails[i]\n",
    "        sentences = sent_tokenize(email)\n",
    "        for j in reversed(range(len(sentences))):\n",
    "            sent = sentences[j]\n",
    "            sentences[j] = sent.strip()\n",
    "            if sent == '':\n",
    "                sentences.pop(j)\n",
    "        emails[i] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m8zau-3yMcM"
   },
   "source": [
    "### 3. Skip-thought encoding\n",
    "Skip-thought encoding, due to [Kiros et al.](https://arxiv.org/abs/1506.06726), is a pre-trained encoder-decoder model that maps sentences to vectors and then predicts similar sentences. Because the skip-thought model is trained on a large [corpus](https://arxiv.org/abs/1506.06724), it can predict semantically similar sentences using words not found in the original encoded text, making it capable of *abstractive* summarization. \n",
    "\n",
    "This implementation relies on a version of `skip-thoughts` ported to Python 3 by Chiao An Yang: https://github.com/tartarskunk/skip-thoughts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n3EmIqm9x0dH"
   },
   "outputs": [],
   "source": [
    "def skipthought_encode(emails):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the emails\n",
    "    \"\"\"\n",
    "    enc_emails = [None]*len(emails)\n",
    "    cum_sum_sentences = [0]\n",
    "    sent_count = 0\n",
    "    for email in emails:\n",
    "      sent_count += len(email)\n",
    "      cum_sum_sentences.append(sent_count)\n",
    "\n",
    "    all_sentences = [sent for email in emails for sent in email]\n",
    "    print('Loading pre-trained models...')\n",
    "    model = skipthoughts.load_model()\n",
    "    encoder = skipthoughts.Encoder(model)\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = encoder.encode(all_sentences, verbose=False)\n",
    "\n",
    "    for i in range(len(emails)):\n",
    "      begin = cum_sum_sentences[i]\n",
    "      end = cum_sum_sentences[i+1]\n",
    "      enc_emails[i] = enc_sentences[begin:end]\n",
    "    return enc_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdICm_4AyeOX"
   },
   "source": [
    "### 4. Clustering\n",
    "The *k*-means method is used to cluster the sentences, encoded as skip-thought vectors, for each email. The distance metric is Euclidean and there are $\\sqrt{\\text{#(sentences in email)}}$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l037YkBt1fyN"
   },
   "source": [
    "### 5. Summarizaton\n",
    "\n",
    "For each cluster, the sentence closest to the mean of the cluster is selected as a representative of the cluster. The summary of an email contains one representative sentance per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tyY2E3FNx2L2"
   },
   "outputs": [],
   "source": [
    "def summarize(emails):\n",
    "    \"\"\"\n",
    "    Performs summarization of emails\n",
    "    \"\"\"\n",
    "    n_emails = len(emails)\n",
    "    summary = [None]*n_emails\n",
    "    print('Preprocesing...')\n",
    "    preprocess(emails)\n",
    "    print('Splitting into sentences...')\n",
    "    split_sentences(emails)\n",
    "    print(emails)\n",
    "    print('Starting to encode...')\n",
    "    enc_emails = skipthought_encode(emails)\n",
    "    print('Encoding Finished')\n",
    "    for i in range(n_emails):\n",
    "        enc_email = enc_emails[i]\n",
    "        n_clusters = int(np.ceil(len(enc_email)**0.5))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        kmeans = kmeans.fit(enc_email)\n",
    "        avg = []\n",
    "        closest = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n",
    "                                                   enc_email)\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        summary[i] = ' '.join([emails[i][closest[idx]] for idx in ordering])\n",
    "    print('Clustering Finished')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVihCXg62U9U"
   },
   "source": [
    "## Verification\n",
    "\n",
    "In this part, we run the summarization pipeline on email data and subjectively evaluate the results. If we had access to a dataset with human-generated summaries, we could treat them as ground truth and use ROUGE-2 scoring to obtain a qualitative measure of machine-generated summary quality. We could then use the ROUGE-2 scores of the unsupervised method as a benchmark to compare with results from deep supervised learning.\n",
    "\n",
    "Another (less-than-ideal) option is to use these unsupervised summaries as labels to train a deep learning model. However, the performace of the DL model would then be limited by the label quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lbXpYi3I-MV5",
    "outputId": "8fc57a01-eee6-497a-95f9-6d9c235df801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesing...\n",
      "Splitting into sentences...\n",
      "[['Brenda, I checked my records and I mailed check #1178 for the normal amount on August 28th.', 'I mailed it to 4303 Pate Rd.', '#29, College Station, TX 77845.', 'I will go ahead and mail you another check.', 'If the first one shows up you can treat the 2nd as payment for October.', 'I know your concerns about the site plan.', 'I will not proceed without getting the details and getting your approval.', 'I will find that amortization schedule and send it soon.', 'Phillip']]\n",
      "Starting to encode...\n",
      "Loading pre-trained models...\n",
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "Encoding sentences...\n",
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "stmt is neither a string nor callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec10ed446371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import pstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    230\u001b[0m            number=default_number, globals=None):\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stmt, setup, timer, globals)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mstmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_stmt()'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stmt is neither a string nor callable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m  \u001b[0;31m# Save for traceback display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: stmt is neither a string nor callable"
     ]
    }
   ],
   "source": [
    "sample = 'sample_email.txt'\n",
    "with open(sample, 'r') as file:\n",
    "    email = file.read()\n",
    "\n",
    "# summarize([email])\n",
    "# type(email)\n",
    "from timeit import timeit\n",
    "# import pstats\n",
    "timeit(summarize([email]), number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline on CounselChat data\n",
    "\n",
    "This represents the baseline. We will improve on this extractive summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_utils import csvtonumpy, get_important_info, clean\n",
    "cc_data = csvtonumpy('datasets/counselchat-data.csv')\n",
    "cc_data = get_important_info(cc_data)\n",
    "cc_data = clean(cc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = cc_data[:,2]\n",
    "ans = cc_data[:20,2]\n",
    "\n",
    "summarize(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS230_milestone.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
